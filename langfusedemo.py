import os
from langfuse.decorators import observe, langfuse_context
from mlx_lm import load, generate
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(".env")

# --- æ¨¡å—åŒ–å‡½æ•° 1: æ£€ç´¢ç¯èŠ‚ ---
@observe(name="retrieval")
def retrieve_context(question):
    # æ¨¡æ‹Ÿæ£€ç´¢é€»è¾‘
    docs = [{"page_content": "MLX utilizes Unified Memory to eliminate data copy between CPU and GPU."}] 
    context = "\n".join([d["page_content"] for d in docs])
    
    # v2 æ ‡å‡†å†™æ³•ï¼šæ›´æ–°å½“å‰ Span çš„å…ƒæ•°æ®
    langfuse_context.update_current_observation(
        metadata={"num_docs": len(docs), "retrieval_type": "vector_search"}
    )
    return context

# --- æ¨¡å—åŒ–å‡½æ•° 2: ç”Ÿæˆç¯èŠ‚ ---
@observe(name="generation") # ç§»é™¤ä¸æ”¯æŒçš„ as_generation=True
def generate_response(prompt, model, tokenizer):
    # æ‰§è¡Œ MLX æ¨ç†
    response = generate(model, tokenizer, prompt=prompt, max_tokens=512)
    
    # æ ¸å¿ƒä¿®æ­£ï¼šæ‰‹åŠ¨æŒ‡å®šè¿™æ˜¯ä¸€ä¸ª Generationï¼Œå¹¶ä¼ å…¥ Input/Output
    langfuse_context.update_current_observation(
        input=prompt,
        output=response,
        model="DeepSeek-R1-7B-MLX",
        metadata={"usage_type": "mlx_inference"}
    )
    return response

# --- ä¸»æµç¨‹ ---
@observe() 
def local_rag_with_tracing(question, model, tokenizer):
    # è®¾ç½®ä¸» Trace çš„å…ƒæ•°æ®
    langfuse_context.update_current_trace(
        name="Local_RAG_Query_V2", 
        user_id="user_123"
    )
    
    # è°ƒç”¨å­å‡½æ•°
    context = retrieve_context(question)
    prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
    response = generate_response(prompt, model, tokenizer)
    
    return response

if __name__ == "__main__":
    print("â³ Loading MLX Model...")
    model_path = "mlx-community/DeepSeek-R1-Distill-Qwen-7B-4bit"
    model, tokenizer = load(model_path)

    print("ğŸš€ Running RAG with Tracing...")
    try:
        ans = local_rag_with_tracing("MLX çš„å†…å­˜ç®¡ç†æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ", model, tokenizer)
        print(f"\nğŸ¤– AI å›ç­”: {ans}")
    finally:
        print("\nğŸ“¤ Syncing traces to Langfuse v2...")
        langfuse_context.flush()